@article{PORNPRASIT2024107523,
title = {Fine-tuning and prompt engineering for large language models-based code review automation},
journal = {Information and Software Technology},
volume = {175},
pages = {107523},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107523},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924001289},
author = {Chanathip Pornprasit and Chakkrit Tantithamthavorn},
keywords = {Modern code review, Code review automation, Large language models, GPT-3.5, Few-shot learning, Persona},
abstract = {Context:
The rapid evolution of Large Language Models (LLMs) has sparked significant interest in leveraging their capabilities for automating code review processes. Prior studies often focus on developing LLMs for code review automation, yet require expensive resources, which is infeasible for organizations with limited budgets and resources. Thus, fine-tuning and prompt engineering are the two common approaches to leveraging LLMs for code review automation.
Objective:
We aim to investigate the performance of LLMs-based code review automation based on two contexts, i.e., when LLMs are leveraged by fine-tuning and prompting. Fine-tuning involves training the model on a specific code review dataset, while prompting involves providing explicit instructions to guide the model’s generation process without requiring a specific code review dataset.
Methods:
We leverage model fine-tuning and inference techniques (i.e., zero-shot learning, few-shot learning and persona) on LLMs-based code review automation. In total, we investigate 12 variations of two LLMs-based code review automation (i.e., GPT-3.5 and Magicoder), and compare them with the Guo et al.’s approach and three existing code review automation approaches (i.e., CodeReviewer, TufanoT5 and D-ACT).
Results:
The fine-tuning of GPT 3.5 with zero-shot learning helps GPT-3.5 to achieve 73.17%–74.23% higher EM than the Guo et al.’s approach. In addition, when GPT-3.5 is not fine-tuned, GPT-3.5 with few-shot learning achieves 46.38%–659.09% higher EM than GPT-3.5 with zero-shot learning.
Conclusions:
Based on our results, we recommend that (1) LLMs for code review automation should be fine-tuned to achieve the highest performance.; and (2) when data is not sufficient for model fine-tuning (e.g., a cold-start problem), few-shot learning without a persona should be used for LLMs for code review automation. Our findings contribute valuable insights into the practical recommendations and trade-offs associated with deploying LLMs for code review automation.}
}
@misc{shin2025promptengineeringfinetuningempirical,
      title={Prompt Engineering or Fine-Tuning: An Empirical Assessment of LLMs for Code},
      author={Jiho Shin and Clark Tang and Tahmineh Mohati and Maleknaz Nayebi and Song Wang and Hadi Hemmati},
      year={2025},
      eprint={2310.10508},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2310.10508},
}
@Article{make6010018,
AUTHOR = {Trad, Fouad and Chehab, Ali},
TITLE = {Prompt Engineering or Fine-Tuning? A Case Study on Phishing Detection with Large Language Models},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {6},
YEAR = {2024},
NUMBER = {1},
PAGES = {367--384},
URL = {https://www.mdpi.com/2504-4990/6/1/18},
ISSN = {2504-4990},
ABSTRACT = {Large Language Models (LLMs) are reshaping the landscape of Machine Learning (ML) application development. The emergence of versatile LLMs capable of undertaking a wide array of tasks has reduced the necessity for intensive human involvement in training and maintaining ML models. Despite these advancements, a pivotal question emerges: can these generalized models negate the need for task-specific models? This study addresses this question by comparing the effectiveness of LLMs in detecting phishing URLs when utilized with prompt-engineering techniques versus when fine-tuned. Notably, we explore multiple prompt-engineering strategies for phishing URL detection and apply them to two chat models, GPT-3.5-turbo and Claude 2. In this context, the maximum result achieved was an F1-score of 92.74% by using a test set of 1000 samples. Following this, we fine-tune a range of base LLMs, including GPT-2, Bloom, Baby LLaMA, and DistilGPT-2—all primarily developed for text generation—exclusively for phishing URL detection. The fine-tuning approach culminated in a peak performance, achieving an F1-score of 97.29% and an AUC of 99.56% on the same test set, thereby outperforming existing state-of-the-art methods. These results highlight that while LLMs harnessed through prompt engineering can expedite application development processes, achieving a decent performance, they are not as effective as dedicated, task-specific LLMs.},
DOI = {10.3390/make6010018}
}
@Article{zhang2024comparison,
AUTHOR = {Zhang X, Talukdar N, Vemulapalli S, Ahn S, Wang J, Meng H, Bin Murtaza SM, Leshchiner D, Dave AA, Joseph DF, Witteveen-Lane M, Chesla D, Zhou J, Chen B},
TITLE = {Comparison of Prompt Engineering and Fine-Tuning Strategies in Large Language Models in the Classification of Clinical Notes},
JOURNAL = {AMIA Joint Summits on Translational Science Proceedings},
YEAR = {2024},
DATE = {May 31},
PAGES = {478--487},
PMID = {38827053},
PMCID = {PMC11141826}
}
@misc{addlesee2023multipartygoaltrackingllms,
      title={Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering},
      author={Angus Addlesee and Weronika Sieińska and Nancie Gunson and Daniel Hernández Garcia and Christian Dondrup and Oliver Lemon},
      year={2023},
      eprint={2308.15231},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.15231},
}
@Article{maharjan2024openmedlm,
AUTHOR = {Maharjan, J., Garikipati, A., Singh, N.P. et al.},
TITLE = {OpenMedLM: prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models.},
JOURNAL = {Scientific Reports},
VOLUME = {14},
NUMBER = {1},
PAGES = {14156},
YEAR = {2024},
DOI = {https://doi.org/10.1038/s41598-024-64827-6}
}
@article{llama3modelcard,

      title={Llama 3 Model Card},

      author={AI@Meta},

      year={2024},

      url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}

}
@Article{arxiv2024language,
      AUTHOR = {Inacio Vieira, Will Allred, Séamus Lankford, Sheila Castilho, Andy Way},
      TITLE = {How Much Data is Enough Data? Fine-Tuning Large Language Models for In-House Translation: Performance Evaluation Across Multiple Dataset Sizes},
      JOURNAL = {arXiv preprint arXiv:2409.03454},
      YEAR = {2024},
      DOI = {https://doi.org/10.48550/arXiv.2409.03454},
      ARCHIVEPREFIX = {arXiv},
      EPRINT = {2409.03454},
      PRIMARYCLASS = {cs.CL}
}

@Article{sui2024tap4llm,
      AUTHOR = {Yuan Sui, Jiaru Zou, Mengyu Zhou, Xinyi He, Lun Du, Shi Han, Dongmei Zhang},
      TITLE = {TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning},
      JOURNAL = {arXiv preprint arXiv:2405.14228},
      YEAR = {2024},
      DOI = {https://doi.org/10.48550/arXiv.2405.14228},
      ARCHIVEPREFIX = {arXiv},
      EPRINT = {2405.14228},
      PRIMARYCLASS = {cs.CL}
}
@Article{liu2024autodw,
      AUTHOR = {Lei Liu, So Hasegawa, Shailaja Keyur Sampat, Maria Xenochristou, Wei-Peng Chen, Takashi Kato, Taisei Kakibuchi, Tatsuya Asai},
      TITLE = {AutoDW: Automatic Data Wrangling Leveraging Large Language Models},
      JOURNAL = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering (ASE 2024)},
      YEAR = {2024},
      DOI = {https://doi.org/10.1145/3691620.3695267},
      ARCHIVEPREFIX = {ACM},
      EPRINT = {3691620.3695267},
      PRIMARYCLASS = {cs.SE}
}
@Article{dohmen2024schemapile,
      AUTHOR = {Till Döhmen, Radu Geacu, Madelon Hulsebos, Sebastian Schelter},
      TITLE = {SchemaPile: A Large Collection of Relational Database Schemas},
      JOURNAL = {Proceedings of the ACM on Management of Data},
      YEAR = {2024},
      VOLUME = {2},
      NUMBER = {3},
      PAGES = {Article 172},
      DOI = {https://doi.org/10.1145/3654975},
      ARCHIVEPREFIX = {ACM},
      EPRINT = {3654975},
      PRIMARYCLASS = {cs.DB}
}
@Article{iravani2024tabletext,
      AUTHOR = {Sahar Iravani, Tim O. F. Conrad},
      TITLE = {Towards More Effective Table-to-Text Generation: Assessing In-Context Learning and Self-Evaluation with Open-Source Models},
      JOURNAL = {arXiv preprint},
      YEAR = {2024},
      VOLUME = {abs/2410.12878},
      NUMBER = {1},
      PAGES = {1--16},
      DOI = {https://arxiv.org/abs/2410.12878},
      ARCHIVEPREFIX = {arXiv},
      EPRINT = {2410.12878},
      PRIMARYCLASS = {cs.CL}
}
@Article{mistral2023mixtral,
      AUTHOR = {Mistral AI team},
      TITLE = {Mixtral of Experts: A High Quality Sparse Mixture-of-Experts},
      JOURNAL = {Mistral AI Research Blog},
      YEAR = {2023},
      VOLUME = {1},
      NUMBER = {1},
      PAGES = {1--1},
      DOI = {https://mistral.ai/news/mixtral-of-experts},
      ARCHIVEPREFIX = {Web},
      EPRINT = {mixtral-of-experts},
      PRIMARYCLASS = {cs.AI}
}
@Article{farabet2025gemma3,
      AUTHOR = {Clement Farabet, Tris Warkentin},
      TITLE = {Introducing Gemma 3: The Most Capable Model You Can Run on a Single GPU or TPU},
      JOURNAL = {Google Developers Blog},
      YEAR = {2025},
      VOLUME = {1},
      NUMBER = {1},
      PAGES = {1--1},
      DOI = {https://blog.google/technology/developers/gemma-3},
      ARCHIVEPREFIX = {Web},
      EPRINT = {gemma-3},
      PRIMARYCLASS = {cs.CL}
}
@Article{gemma2025technical,
      AUTHOR = {Gemma Team},
      TITLE = {Gemma 3 Technical Report},
      JOURNAL = {arXiv preprint arXiv:2503.19786},
      YEAR = {2025},
      DOI = {https://doi.org/10.48550/arXiv.2503.19786},
      ARCHIVEPREFIX = {arXiv},
      EPRINT = {2503.19786},
      PRIMARYCLASS = {cs.CL}
}
@Article{ilhan2024gemma,
      AUTHOR = {Şevval İlhan},
      TITLE = {Introducing Google Gemma: The Accessible Open-Source AI Solution},
      JOURNAL = {Medium},
      YEAR = {2024},
      MONTH = {February},
      URL = {https://medium.com/@ilhnsevval/introducing-google-gemma-the-accessible-open-source-ai-solution-8b352fa65650},
}
@Article{alibaba2025qwen,
      AUTHOR = {Alibaba Cloud},
      TITLE = {Qwen LLMs},
      JOURNAL = {Alibaba Cloud Documentation},
      YEAR = {2025},
      MONTH = {February},
      URL = {https://www.alibabacloud.com/help/en/model-studio/what-is-qwen-llm}
}
@Article{bilenko2024phi3,
      AUTHOR = {Misha Bilenko},
      TITLE = {New models added to the Phi-3 family, available on Microsoft Azure},
      JOURNAL = {Microsoft Azure Blog},
      YEAR = {2024},
      MONTH = {May},
      DAY = {21},
      URL = {https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/}
}
@Article{deepseek2025r1,
      AUTHOR = {DeepSeek-AI},
      TITLE = {DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
      JOURNAL = {arXiv preprint arXiv:2501.12948},
      YEAR = {2025},
      MONTH = {January},
      DAY = {22},
      ARCHIVEPREFIX = {arXiv},
      EPRINT = {2501.12948},
      PRIMARYCLASS = {cs.CL},
      NOTE = {Available at \url{https://arxiv.org/abs/2501.12948}}
}
@Article{granite2025v3.2,
      AUTHOR = {Granite Team, IBM},
      TITLE = {Granite-3.2-8B-Instruct: An 8-Billion-Parameter Long-Context AI Model with Controllable Thinking Capability},
      JOURNAL = {Granite Docs},
      YEAR = {2025},
      MONTH = {February},
      DAY = {26},
      URL = {https://huggingface.co/ibm-granite/granite-3.2-8b-instruct},
      NOTE = {Apache 2.0 License}
}
@misc{qwen3technicalreport,
      title={Qwen3 Technical Report},
      author={Qwen Team},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388},
}
@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948},
}
@article{dint_a_00201,
      author = {Wu, Xindong and Chen, Hao and Bu, Chenyang and Ji, Shengwei and Zhang, Zan and Sheng, Victor S.},
      title = {HUSS: A Heuristic Method for Understanding the Semantic Structure of Spreadsheets},
      journal = {Data Intelligence},
      volume = {5},
      number = {3},
      pages = {537-559},
      year = {2023},
      month = {08},
      abstract = {Spreadsheets contain a lot of valuable data and have many practical applications. The key technology of these practical applications is how to make machines understand the semantic structure of spreadsheets, e.g., identifying cell function types and discovering relationships between cell pairs. Most existing methods for understanding the semantic structure of spreadsheets do not make use of the semantic information of cells. A few studies do, but they ignore the layout structure information of spreadsheets, which affects the performance of cell function classification and the discovery of different relationship types of cell pairs. In this paper, we propose a Heuristic algorithm for Understanding the Semantic Structure of spreadsheets (HUSS). Specifically, for improving the cell function classification, we propose an error correction mechanism (ECM) based on an existing cell function classification model [11] and the layout features of spreadsheets. For improving the table structure analysis, we propose five types of heuristic rules to extract four different types of cell pairs, based on the cell style and spatial location information. Our experimental results on five real-world datasets demonstrate that HUSS can effectively understand the semantic structure of spreadsheets and outperforms corresponding baselines.},
      issn = {2641-435X},
      doi = {10.1162/dint_a_00201},
      url = {https://doi.org/10.1162/dint\_a\_00201},
      eprint = {https://direct.mit.edu/dint/article-pdf/5/3/537/2158253/dint\_a\_00201.pdf},
}
@article{milovsevic2018multi,
      title={A multi-layered approach to information extraction from tables in biomedical documents},
      author={Milo{\v{s}}evic, Nikola},
      year={2018}
}
@inproceedings{fang2012table,
      title={Table header detection and classification},
      author={Fang, Jing and Mitra, Prasenjit and Tang, Zhi and Giles, C Lee},
      booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
      volume={26},
      number={1},
      pages={599--605},
      year={2012}
}
@inproceedings{engels2018rule,
      title={A rule-based approach to analyzing database schema objects with datalog},
      author={Engels, Christiane and Behrend, Andreas and Brass, Stefan},
      booktitle={Logic-Based Program Synthesis and Transformation: 27th International Symposium, LOPSTR 2017, Namur, Belgium, October 10-12, 2017, Revised Selected Papers 27},
      pages={20--36},
      year={2018},
      organization={Springer}
}
@inproceedings{koci2016machine,
      title={A machine learning approach for layout inference in spreadsheets},
      author={Koci, Elvis and Thiele, Maik and Romero, Oscar and Lehner, Wolfgang},
      booktitle={International Conference on Knowledge Discovery and Information Retrieval},
      volume={2},
      pages={77--88},
      year={2016},
      organization={SciTePress}
}
@article{budhiraja2020supervised,
      title={A supervised learning approach for heading detection},
      author={Budhiraja, Sahib Singh and Mago, Vijay},
      journal={Expert systems},
      volume={37},
      number={4},
      pages={e12520},
      year={2020},
      publisher={Wiley Online Library}
}
@inproceedings{inproceedings,
      author = {Döhmen, Till and Mühleisen, Hannes and Boncz, Peter},
      year = {2017},
      month = {06},
      pages = {1-12},
      title = {Multi-Hypothesis CSV Parsing},
      doi = {10.1145/3085504.3085520}
}
@article{bai2024schema,
      title={Schema-Driven Information Extraction from Heterogeneous Tables},
      author={Bai, Fan and Kang, Junmo and Stanovsky, Gabriel and Freitag, Dayne and Ritter, Alan},
      journal={arXiv preprint arXiv:2305.14336v3},
      year={2024},
      url={http://arxiv.org/pdf/2305.14336v3},
      archivePrefix={arXiv},
      eprint={2305.14336v3},
      primaryClass={cs.CL}
}
@misc{lijin2024structured,
      author = {Sam Lijin},
      title = {Every Way To Get Structured Output From LLMs},
      year = {2024},
      month = nov,
      url = {https://www.boundaryml.com/blog/structured-output-from-llms},
      note = {Accessed: 2025-06-12}
}

@article{deepseek2024deepseek,
      title={DeepSeek-R1: The First Fully Open Source RAG Language Model Suite},
      author={DeepSeek AI},
      journal={arXiv preprint arXiv:2501.12948},
      year={2024},
      url={https://arxiv.org/abs/2501.12948}
}
@article{turn0search7,
      title={Multimodal Table Understanding},
      author={Zhang et al.},
      year={2024},
      note={arXiv preprint 2406.08100}
}

@article{turn0academia12,
      title={PubTables‑1M: Towards comprehensive table extraction from unstructured documents},
      author={Smock, Pesala, Abraham},
      year={2021},
      note={arXiv preprint}
}

@article{turn0academia13,
      title={TabLeX: A Benchmark Dataset for Structure and Content Information Extraction from Scientific Tables},
      author={Desai, Kayal, Singh},
      year={2021},
      note={arXiv preprint}
}

@article{turn0search2,
      title={Knowledge-Aware Reasoning over Multimodal Semi-structured Tables},
      author={Anonymous},
      year={2024},
      note={arXiv preprint 2408.13860}
}

@article{turn0search5,
      title={Aligning benchmark datasets for table structure recognition},
      author={Smock et al.},
      year={2023},
      note={research article}
}

@misc{turn0search0,
      title={upstage/dp-bench dataset},
      author={UpStage},
      year={2025},
      note={Hugging Face dataset}
}
@misc{turn0search11,
      title={Why is table extraction still not solved by modern multimodal models?},
      author={Reddit user Healthy-Nebula-3603},
      year={2025},
      note={Reddit discussion}
}
@article{turn0search8,
      title={Prompt Engineering or Fine Tuning: An Empirical Assessment of LLMs for Code (OpenReview)},
      author={Shin, Jiho et al.},
      year={2023},
      note={OpenReview CoRR}
}

@article{heltweg2025empirical,
      author       = {Philip Heltweg and Georg‐Daniel Schwarz and Dirk Riehle and Felix Quast},
      title        = {An Empirical Study on the Effects of Jayvee, a Domain‑Specific Language for Data Engineering, on Understanding Data Pipeline Architectures},
      journal      = {Software: Practice \& Experience},
      volume       = {55},
      number       = {6},
      pages        = {1086--1105},
      year         = {2025},
      doi          = {10.1002/spe.3409}
}

@article{yang2021autopipeline,
      author       = {Junwen Yang and Yeye He and Surajit Chaudhuri},
      title        = {Auto‑Pipeline: Synthesizing Complex Data Pipelines By‑Target Using Reinforcement Learning and Search},
      journal      = {Proceedings of the VLDB Endowment},
      volume       = {14},
      number       = {11},
      pages        = {2563--2575},
      year         = {2021},
      doi          = {10.14778/3476249.3476303}
}

@techreport{krishnan2019alphaclean,
      author       = {Sanjay Krishnan and Eugene Wu},
      title        = {AlphaClean: Automatic Generation of Data Cleaning Pipelines},
      institution  = {CoRR},
      volume       = {abs/1904.11827},
      year         = {2019},
      url          = {https://arxiv.org/abs/1904.11827}
}

@inproceedings{feng2017component,
      author       = {Yu Feng and Ruben Martins and Jacob Van Geffen and Isil Dillig and Swarat Chaudhuri},
      title        = {Component‑based Synthesis of Table Consolidation and Transformation Tasks from Examples},
      booktitle    = {Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation},
      pages        = {55–68},
      year         = {2017},
      doi          = {10.1145/3062341.3062351}
}

@inproceedings{ye2020variational,
      author       = {Rong Ye and Wenxian Shi and Hao Zhou and Zhongyu Wei and Lei Li},
      title        = {Variational Template Machine for Data‑to‑Text Generation},
      booktitle    = {International Conference on Learning Representations (ICLR)},
      year         = {2020},
      url          = {https://openreview.net/forum?id=HkejNgBtPB}
}





